import asyncio
import sys
from google import genai
import os
import json
import time
import pandas as pd
from dotenv import load_dotenv
from google import genai
from reportlab.lib.pagesizes import A4, landscape
from reportlab.lib import colors
from reportlab.lib.styles import getSampleStyleSheet
from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer
from reportlab.lib import colors
from reportlab.lib.styles import ParagraphStyle

# === è¨­å®š ===
csv_file_path = "japanese_pretest.csv"
jlpt_file_path = "japanese_learning_info.csv"
chunk_size = 1000
batch_size = 12
delimiter = "-----"

ITEMS = [
]

# === è¼‰å…¥ API Key ===
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# === JSON Parse Function ===
def parse_response(response_text):
    cleaned = response_text.strip()
    if cleaned.startswith("```"):
        lines = cleaned.splitlines()
        if lines[0].startswith("```"):
            lines = lines[1:]
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        cleaned = "\n".join(lines).strip()

    try:
        result = json.loads(cleaned)
        for item in ITEMS:
            if item not in result:
                result[item] = ""
        return result
    except Exception as e:
        print(f"âš ï¸ è§£æ JSON å¤±æ•—ï¼š{e}")
        print("åŸå§‹å›å‚³å…§å®¹ï¼š", response_text)
        return {item: "" for item in ITEMS}

# === Dialogue Column Detection ===
def select_dialogue_column(chunk: pd.DataFrame) -> str:
    preferred = ["text", "utterance", "content", "dialogue", "Dialogue"]
    for col in preferred:
        if col in chunk.columns:
            return col
    print("âš ï¸ æ‰¾ä¸åˆ°é è¨­æ¬„ä½ï¼Œå°‡ä½¿ç”¨ç¬¬ä¸€å€‹æ¬„ä½ï¼š", list(chunk.columns))
    return chunk.columns[0]

# === Classify by Score ===
def classify_level(score):
    if score < 45:
        return "Beginner"
    elif 45 <= score < 55:
        return "Intermediate"
    else:
        return "Advanced"

# === å°‡å­¸ç”Ÿåˆ†ç´šä¸¦è¼¸å‡ºåˆ†é¡çµæœ ===
def classify_and_export():
    pretest_df = pd.read_csv(csv_file_path)
    jlpt_df = pd.read_csv(jlpt_file_path)
    merged_df = pd.merge(pretest_df, jlpt_df, on="StudentID")
    merged_df["Class_Level"] = merged_df["Total_Score"].apply(classify_level)

    class_groups = {
        "Beginner": merged_df[merged_df["Class_Level"] == "Beginner"],
        "Intermediate": merged_df[merged_df["Class_Level"] == "Intermediate"],
        "Advanced": merged_df[merged_df["Class_Level"] == "Advanced"]
    }

    for level, df in class_groups.items():
        filename = f"class_{level.lower()}.csv"
        df.to_csv(filename, index=False)

    return merged_df

# === æ‰¹æ¬¡åˆ†æé€å­—ç¨¿ ===
def process_batch_dialogue(client, dialogues: list):
    prompt = (
        "ä½ æ˜¯ä¸€ä½æ—¥æ–‡æ•™å­¸å°ˆå®¶ï¼Œè«‹æ ¹æ“šä»¥ä¸‹ç·¨ç¢¼è¦å‰‡è©•ä¼°æ˜¯å¦èˆ‡æ—¥èªæ•™å­¸çš„ç›®æ¨™ç›¸åŒï¼š\n"
        + "\n".join(ITEMS) +
        "\n\nè«‹ä¾æ“šè©•ä¼°çµæœï¼Œå°æ¯å€‹é …ç›®ï¼šè‹¥è§¸åŠå‰‡æ¨™è¨˜ç‚º Yesï¼Œå¦å‰‡æ¨™è¨˜ç‚º Noã€‚"
        "è«‹å°æ¯ç­†é€å­—ç¨¿ç”¢ç”Ÿ JSON æ ¼å¼å›è¦†ï¼Œä¸¦åœ¨å„ç­†çµæœé–“ç”¨ä¸‹åˆ—åˆ†éš”ç·šéš”é–‹ï¼š\n"
        f"{delimiter}\n"
        "ä¾‹å¦‚ï¼š\n"
        "```json\n"
        "{ \"Vocab\": \"Yes\", \"Listening\": \"No\", ... }\n"
        f"{delimiter}\n"
        "{...}\n```"
    )

    batch_text = f"\n{delimiter}\n".join(dialogues)
    content = prompt + "\n\n" + batch_text

    try:
        response = client.generate_content(
            model="models/gemini-1.5-flash-latest",
            contents=[{"role": "user", "parts": [content]}]
        )
    except Exception as e:
        print(f"âŒ API å‘¼å«å¤±æ•—ï¼š{e}")
        return [{item: "" for item in ITEMS} for _ in dialogues]

    response_text = response.text if hasattr(response, "text") else response.candidates[0].content.parts[0].text
    print("âœ… æ‰¹æ¬¡ API å›å‚³å…§å®¹ï¼š", response_text)

    parts = response_text.split(delimiter)
    results = []
    for part in parts:
        part = part.strip()
        if part:
            results.append(parse_response(part))

    while len(results) < len(dialogues):
        results.append({item: "" for item in ITEMS})
    return results[:len(dialogues)]

# === ä¸»ç¨‹å¼ ===
if __name__ == "__main__":
    # === Part 1: å­¸ç”Ÿåˆ†ç´š ===
    merged_df = classify_and_export()

    # === Part 2 & 3: è™•ç†æ¯å€‹ç´šåˆ¥ ===
    for level in ["beginner", "intermediate", "advanced"]:
        input_csv = f"class_{level}.csv"
        output_csv = f"output_{level}.csv"

        if not os.path.exists(input_csv):
            print(f"âš ï¸ æ‰¾ä¸åˆ°æª”æ¡ˆï¼š{input_csv}")
            continue

        df = pd.read_csv(input_csv)
        dialogue_col = select_dialogue_column(df)
        print(f"ğŸ” [{level.capitalize()}] ä½¿ç”¨æ¬„ä½ä½œç‚ºé€å­—ç¨¿ä¾†æºï¼š{dialogue_col}")

        if os.path.exists(output_csv):
            os.remove(output_csv)

        total = len(df)
        for start_idx in range(0, total, batch_size):
            end_idx = min(start_idx + batch_size, total)
            batch = df.iloc[start_idx:end_idx]
            dialogues = [str(d).strip() for d in batch[dialogue_col]]
            batch_results = process_batch_dialogue(genai, dialogues)

            batch_df = batch.copy()
            for item in ITEMS:
                batch_df[item] = [res.get(item, "") for res in batch_results]

            mode = 'w' if start_idx == 0 else 'a'
            header = (start_idx == 0)
            batch_df.to_csv(output_csv, mode=mode, index=False, header=header, encoding="utf-8-sig")
            print(f"âœ… [{level.capitalize()}] å·²è™•ç† {end_idx}/{total} ç­†è³‡æ–™")

            time.sleep(1)

        print(f"ğŸ‰ [{level.capitalize()}] å…¨éƒ¨è™•ç†å®Œæˆï¼Œçµæœå„²å­˜æ–¼ï¼š{output_csv}")


def create_pdf_from_csv(csv_file, pdf_file):
    df = pd.read_csv(csv_file)

    doc = SimpleDocTemplate(pdf_file, pagesize=landscape(A4))  # ä½¿ç”¨æ©«å¼ A4
    styles = getSampleStyleSheet()
    elements = []

    title = Paragraph(f"<b>Class Report: {os.path.splitext(os.path.basename(csv_file))[0].title()}</b>", styles["Title"])
    elements.append(title)
    elements.append(Spacer(1, 12))

    data = [df.columns.tolist()] + df.values.tolist()

    formatted_data = []
    for row in data:
        formatted_row = [Paragraph(str(cell), styles["Normal"]) for cell in row]
        formatted_data.append(formatted_row)

    table = Table(formatted_data, repeatRows=1)
    table.setStyle(TableStyle([
    ("BACKGROUND", (0, 0), (-1, 0), colors.HexColor("#e1e7f0")),
    ("TEXTCOLOR", (0, 0), (-1, 0), colors.whitesmoke),
    ("ALIGN", (0, 0), (-1, -1), "LEFT"),
    ("VALIGN", (0, 0), (-1, -1), "TOP"),
    ("FONTNAME", (0, 0), (-1, 0), "Helvetica-Bold"),
    ("FONTSIZE", (0, 0), (-1, -1), 10),
    ("BOTTOMPADDING", (0, 0), (-1, 0), 10),
    ("TOPPADDING", (0, 0), (-1, -1), 6),
    ("GRID", (0, 0), (-1, -1), 0.25, colors.grey),
    ("ROWBACKGROUNDS", (0, 1), (-1, -1), [colors.whitesmoke, colors.lightgrey]),
]))  

    elements.append(table)
    doc.build(elements)
    print(f"âœ… PDF ç”¢å‡ºæˆåŠŸï¼š{pdf_file}")

# æ‰¹æ¬¡è™•ç†ä¸‰å€‹ç­‰ç´š
for level in ["beginner", "intermediate", "advanced"]:
    csv = f"output_{level}.csv"
    pdf = f"output_{level}.pdf"
    if os.path.exists(csv):
        create_pdf_from_csv(csv, pdf)

#last

# æ ¹æ“šä½ çš„å°ˆæ¡ˆçµæ§‹èª¿æ•´ä¸‹åˆ— import
from autogen_agentchat.agents import AssistantAgent, UserProxyAgent
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.messages import TextMessage
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.agents.web_surfer import MultimodalWebSurfer

load_dotenv()

async def process_chunk(chunk, start_idx, total_records, model_client, termination_condition):
    """
    è™•ç†å–®ä¸€æ‰¹æ¬¡è³‡æ–™ï¼š
      - å°‡è©²æ‰¹æ¬¡è³‡æ–™è½‰æˆ dict æ ¼å¼
      - çµ„å‡ºæç¤ºï¼Œè¦æ±‚å„ä»£ç†äººæ ¹æ“šè©²æ‰¹æ¬¡è³‡æ–™é€²è¡Œåˆ†æï¼Œ
        ä¸¦æä¾›èªè¨€å­¸ç¿’ä¸­å¿ƒå»ºè­°ã€‚
      - è«‹ MultimodalWebSurfer ä»£ç†äººåˆ©ç”¨å¤–éƒ¨ç¶²ç«™æœå°‹åŠŸèƒ½ï¼Œ
        æœå°‹é—œæ–¼æ—¥èªå­¸ç¿’ä¹‹è©•é‡åŸºæº–ï¼ˆä¾‹å¦‚å£èªªã€å–®å­—é‡ã€æ–‡æ³•ä½¿ç”¨ã€è½åŠ›æŠ€å·§ã€é–±è®€æ¸¬é©—ç­‰ï¼‰ï¼Œ
        ä¸¦å°‡æœå°‹çµæœç´å…¥å»ºè­°ä¸­ã€‚
      - æ”¶é›†æ‰€æœ‰å›è¦†è¨Šæ¯ä¸¦è¿”å›ã€‚
    """
    # å°‡è³‡æ–™è½‰æˆ dict æ ¼å¼
    chunk_data = chunk.to_dict(orient='records')
    prompt = (
        f"ç›®å‰æ­£åœ¨è™•ç†ç¬¬ {start_idx} è‡³ {start_idx + len(chunk) - 1} ç­†è³‡æ–™ï¼ˆå…± {total_records} ç­†ï¼‰ã€‚\n"
        f"ä»¥ä¸‹ç‚ºè©²æ‰¹æ¬¡è³‡æ–™:\n{chunk_data}\n\n"
        "è«‹æ ¹æ“šä»¥ä¸Šè³‡æ–™é€²è¡Œåˆ†æï¼Œä¸¦æä¾›å®Œæ•´çš„å»ºè­°ã€‚"
        "å…¶ä¸­è«‹ç‰¹åˆ¥æ³¨æ„ï¼š\n"
        "  1. åˆ†ç³»å¸‚é¢ä¸ŠåŠç¶²è·¯ä¸Šä¸‰ä»½æ—¥æ–‡æ•™å­¸é€²åº¦ï¼Œä¸¦å€åˆ†åˆå­¸è€…ã€ä¸­ç­‰å­¸ç¿’è€…ã€é«˜ç´šå­¸ç¿’è€…ç­‰ã€‚ï¼›\n"
        "  2. è«‹ MultimodalWebSurfer æœå°‹å¤–éƒ¨ç¶²ç«™ï¼Œæœå°‹é—œæ–¼æ—¥èªå­¸ç¿’ä¹‹è©•é‡åŸºæº–ï¼ˆä¾‹å¦‚å£èªªã€å–®å­—é‡ã€æ–‡æ³•ä½¿ç”¨ã€è½åŠ›æŠ€å·§ã€é–±è®€æ¸¬é©—ç­‰ï¼‰ï¼Œ\n"
        "     ä¸¦å°‡æœå°‹çµæœæ•´åˆé€²å›è¦†ä¸­ï¼›\n"
        "  3. æœ€å¾Œè«‹æä¾›ä¸‰ä»½ä¸åŒç­‰ç´šä¸”æ»¿åˆ†ç‚º100åˆ†çš„æ—¥èªæ¸¬é©—å’Œç›¸é—œåƒè€ƒè³‡è¨Šã€‚\n"
        "è«‹å„ä»£ç†äººå”åŒåˆä½œï¼Œæä¾›ä¸€ä»½å®Œæ•´ä¸”å…·åƒè€ƒåƒ¹å€¼çš„å»ºè­°ä¸¦æ ¹æ“šèª¿æŸ¥å…§å®¹ï¼Œè¨­è¨ˆå‡ºä¸€ä»½å…·é‘‘åˆ¥æ—¥èªå­¸ç¿’è€…çš„æ¸¬é©—è€ƒå·ã€‚"
    )
    
    # ç‚ºæ¯å€‹æ‰¹æ¬¡å»ºç«‹æ–°çš„ agent èˆ‡ team å¯¦ä¾‹
    local_data_agent = AssistantAgent("data_agent", model_client)
    local_web_surfer = MultimodalWebSurfer("web_surfer", model_client)
    local_assistant = AssistantAgent("assistant", model_client)
    local_user_proxy = UserProxyAgent("user_proxy")
    local_team = RoundRobinGroupChat(
        [local_data_agent, local_web_surfer, local_assistant, local_user_proxy],
        termination_condition=termination_condition
    )
    
    messages = []
    async for event in local_team.run_stream(task=prompt):
        if isinstance(event, TextMessage):
            # å°å‡ºç›®å‰å“ªå€‹ agent æ­£åœ¨é‹ä½œï¼Œæ–¹ä¾¿è¿½è¹¤
            print(f"[{event.source}] => {event.content}\n")
            messages.append({
                "batch_start": start_idx,
                "batch_end": start_idx + len(chunk) - 1,
                "source": event.source,
                "content": event.content,
                "type": event.type,
                "prompt_tokens": event.models_usage.prompt_tokens if event.models_usage else None,
                "completion_tokens": event.models_usage.completion_tokens if event.models_usage else None
            })
    return messages

async def main():
    gemini_api_key = os.environ.get("GEMINI_API_KEY")
    if not gemini_api_key:
        print("è«‹æª¢æŸ¥ .env æª”æ¡ˆä¸­çš„ GEMINI_API_KEYã€‚")
        return

    # åˆå§‹åŒ–æ¨¡å‹ç”¨æˆ¶ç«¯ (æ­¤è™•ç¤ºç¯„ä½¿ç”¨ gemini-2.0-flash)
    model_client = OpenAIChatCompletionClient(
        model="gemini-2.0-flash",
        api_key=gemini_api_key,
    )
    
    termination_condition = TextMentionTermination("exit")
    
    # ä½¿ç”¨ pandas ä»¥ chunksize æ–¹å¼è®€å– CSV æª”æ¡ˆ
    csv_file_path1 = "output_advanced.csv"
    csv_file_path2 = "output_beginner.csv"
    csv_file_path3 = "output_intermediate.csv"
    chunk_size = 1000
    chunks = []
    for csv_file_path in [csv_file_path1, csv_file_path2, csv_file_path3]:
        for chunk in pd.read_csv(csv_file_path, chunksize=chunk_size):
            chunks.append(chunk)
    combined_df = pd.concat(chunks, ignore_index=True)
    total_records = sum(chunk.shape[0] for chunk in chunks)
    
    # åˆ©ç”¨ map èˆ‡ asyncio.gather åŒæ™‚è™•ç†æ‰€æœ‰æ‰¹æ¬¡ï¼ˆé¿å…ä½¿ç”¨å‚³çµ± for è¿´åœˆï¼‰
    tasks = list(map(
        lambda idx_chunk: process_chunk(
            idx_chunk[1],
            idx_chunk[0] * chunk_size,
            total_records,
            model_client,
            termination_condition
        ),
        enumerate(chunks)
    ))
    
    results = await asyncio.gather(*tasks)
    # å°‡æ‰€æœ‰æ‰¹æ¬¡çš„è¨Šæ¯å¹³å¦åŒ–æˆä¸€å€‹æ¸…å–®
    all_messages = [msg for batch in results for msg in batch]
    
    # å°‡å°è©±ç´€éŒ„æ•´ç†æˆ DataFrame ä¸¦å­˜æˆ CSV
    df_log = pd.DataFrame(all_messages)
    output_file = "final_conversation.csv"
    df_log.to_csv(output_file, index=False, encoding="utf-8-sig")
    print(f"å·²å°‡æ‰€æœ‰å°è©±ç´€éŒ„è¼¸å‡ºç‚º {output_file}")

if __name__ == '__main__':
    asyncio.run(main())